 
---
title: "Using Ridge Regression with ridgereg and caret"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ridgereg with caret package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(dplyr)
library(mlbench)  # For BostonHousing data
library(packagelinreg)  # Replace with your package name
```

# Introduction

This vignette demonstrates the use of ridge regression with the RidgeRegressor function from the ridgereg package. We will use the BostonHousing dataset to show how to set up a predictive model with ridge regression and compare it to standard linear regression.

The `RidgeRegressor` class implements a ridge regression model, supporting both least-squares and QR decomposition methods for estimating coefficients.

#Load and Split the Data
We will split the BostonHousing data into training and test sets.
```{r}
data("BostonHousing", package = "mlbench")
set.seed(123)

# Split into training (80%) and test (20%) sets
trainIndex <- caret::createDataPartition(BostonHousing$medv, p = 0.8, list = FALSE)
trainData <- BostonHousing[trainIndex, ]
testData <- BostonHousing[-trainIndex, ]

```

#Fit Linear Regression Models
Simple Linear Regression
```{r}
# Fit a simple linear regression model
lm_model <- lm(medv ~ ., data = trainData)
summary(lm_model)
```

#Evaluate Linear Models on Training Data
We will use RMSE and R-squared as evaluation metrics, and define the evaluate_model function here so it can be reused across models.

```{r}
# Function to calculate RMSE and R-squared
evaluate_model <- function(model, data) {
  preds <- predict(model, newdata = data)
  RMSE <- sqrt(mean((data$medv - preds)^2))
  R2 <- 1 - sum((data$medv - preds)^2) / sum((data$medv - mean(data$medv))^2)
  return(list(RMSE = RMSE, R2 = R2))
}

```

#Fit Linear Regression Models
Simple Linear Regression
```{r}
# Fit a simple linear regression model
lm_model <- lm(medv ~ ., data = trainData)
summary(lm_model)
```


Evaluate Linear Models on Training Data
```{r}
# Evaluate the simple linear regression model
lm_results <- evaluate_model(lm_model, trainData)

# Forward selection
lm_forward <- step(lm(medv ~ 1, data = trainData), 
                   scope = ~ .,
                   direction = "forward")
lm_forward_results <- evaluate_model(lm_forward, trainData)

# Display results
lm_results
lm_forward_results

```


#Fit Ridge Regression Model with RidgeRegressor
Using the RidgeRegressor function from ridgereg, we will fit a ridge regression model and explore different values of lambda.

```{r}
# Fit ridge regression with a specified lambda
ridge_model <- RidgeRegressor$new(formula = medv ~ ., data = trainData, lambda = 0.1)
ridge_model$print()

```

#Hyperparameter Tuning for lambda
We will use cross-validation to find the best lambda value.

```{r}
# Define lambda values to test
lambda_values <- seq(0.01, 1, by = 0.05)
cv_results <- sapply(lambda_values, function(l) {
  ridge_model <- RidgeRegressor$new(formula = medv ~ ., data = trainData, lambda = l)
  preds <- ridge_model$predict(newdata = testData)
  sqrt(mean((testData$medv - preds)^2))
})

# Plot cross-validation results
plot(lambda_values, cv_results, type = "b", xlab = "Lambda", ylab = "RMSE")
best_lambda <- lambda_values[which.min(cv_results)]
best_lambda
```

#Evaluate the Ridge Regression Model on Test Data
Finally, we will evaluate the performance of the best ridge regression model.

```{r}
evaluate_model <- function(model, data) {
  # Use the R6 predict method directly
  preds <- model$predict(newdata = data)
  RMSE <- sqrt(mean((data$medv - preds)^2))
  R2 <- 1 - sum((data$medv - preds)^2) / sum((data$medv - mean(data$medv))^2)
  return(list(RMSE = RMSE, R2 = R2))
}

```


```{r}
# Fit final ridge model with the best lambda

final_ridge_model <- RidgeRegressor$new(formula = medv ~ ., data = trainData, lambda = best_lambda)
ridge_results <- evaluate_model(final_ridge_model, testData)
ridge_results

```

#Compare Models and Conclude
Summarize and compare the performance of all models on the test set.

```{r}
comparison <- data.frame(
  Model = c("Linear Regression", "Forward Selection", "Ridge Regression"),
  RMSE = c(lm_results$RMSE, lm_forward_results$RMSE, ridge_results$RMSE),
  R2 = c(lm_results$R2, lm_forward_results$R2, ridge_results$R2)
)
comparison

```

#Conclusion
In this vignette, we demonstrated how to fit and evaluate linear and ridge regression models on the BostonHousing dataset. The results show that:
Linear Regression achieved an RMSE of 4.717 and an R² of 0.735, indicating a reasonably good fit on the data.
Forward Selection performed poorly, with an RMSE of 9.157 and an R² of 0.000, suggesting that this model failed to capture the variance in the dataset effectively, likely due to over-simplification or insufficient feature selection.
Ridge Regression provided the best performance, with the lowest RMSE of 4.563 and the highest R² of 0.760. This indicates that ridge regression effectively reduced overfitting compared to simple linear regression by adding regularization, which helped improve the model’s generalization on unseen data.
